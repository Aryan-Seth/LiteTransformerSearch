{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QuickStart\n",
        "\n",
        "In this Notebook we'll run Archai's [Quickstart](https://microsoft.github.io/archai/getting_started/quick_start.html) example on Azure Machine Learning.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.7 or later\n",
        "- An Azure subscription\n",
        "- An Azure Resource Group\n",
        "- An Azure Machine Learning [Workspace](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources#create-the-workspace)\n",
        "\n",
        "## Requirements\n",
        "\n",
        "Install Azure SDK requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install azure-ai-ml azure-identity jinja2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "from azure.ai.ml import MLClient, command, Output\n",
        "from azure.ai.ml.entities import AmlCompute, Environment\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get a handle to the workspace\n",
        "\n",
        "We load the workspace from a workspace [configuration file](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#local-and-dsvm-only-create-a-workspace-configuration-file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675428777177
        }
      },
      "outputs": [],
      "source": [
        "# Set the path to your config. file\n",
        "path = Path(\"../.azureml/config.json\")\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "ml_client = MLClient.from_config(\n",
        "    credential=credential,\n",
        "    path=path\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a compute cluster\n",
        "\n",
        "We provision a Linux [compute cluster](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-attach-compute-cluster?tabs=python) fos this Notebook. See the [full list](https://azure.microsoft.com/en-ca/pricing/details/machine-learning/) on VM sizes and prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675428792174
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "cpu_compute_name = \"nas-cpu-cluster-D14-v2\"\n",
        "\n",
        "try:\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_name)\n",
        "    print(f\"You already have a cluster named {cpu_compute_name}, we'll reuse it as is.\")\n",
        "\n",
        "except Exception:\n",
        "    cpu_compute = AmlCompute(\n",
        "        name=cpu_compute_name,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"Standard_D14_v2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=4,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_compute).result()\n",
        "    print(f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create an environment based on a YAML file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675428814037
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "custom_env_name = \"aml-archai\"\n",
        "\n",
        "archai_job_env = Environment(\n",
        "    name=custom_env_name,\n",
        "    description=\"Custom environment for Archai Job\",\n",
        "    tags={\"archai\": \"1.0.0\"},\n",
        "    conda_file=\"conda.yaml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
        "    version=\"0.1.0\"\n",
        ")\n",
        "archai_job_env = ml_client.environments.create_or_update(archai_job_env)\n",
        "\n",
        "print(\n",
        "    f\"Environment with name {archai_job_env.name} is registered to workspace, the environment version is {archai_job_env.version}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675429833672
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "job = command(\n",
        "    display_name=\"Archai's QuickStart\",\n",
        "    outputs=dict(\n",
        "        output_path=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
        "    ),\n",
        "    code=\"main.py\",\n",
        "    command=\"python main.py --output_dir ${{outputs.output_path}}\",\n",
        "    environment=f\"{archai_job_env.name}:{archai_job_env.version}\",\n",
        "    compute=cpu_compute_name,\n",
        "    experiment_name=\"archai_quickstart\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Run job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1675429838269
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "quickstart_job = ml_client.create_or_update(job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stream logs of the job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(quickstart_job.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download job's output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completed_job = ml_client.jobs.get(quickstart_job.name)\n",
        "download_path = \"output\"\n",
        "output_name = \"output_path\"\n",
        "\n",
        "if completed_job.status == \"Completed\":\n",
        "    ml_client.jobs.download(name=completed_job.name, download_path=download_path, output_name=output_name)\n",
        "else:\n",
        "    print(\"Job is not completed yet\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Pareto Frontier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "downloaded_folder = Path(download_path) / \"named-outputs\" / output_name\n",
        "\n",
        "param_vs_latency_img = Image(filename=downloaded_folder / \"pareto_non_embedding_params_vs_onnx_latency.png\")\n",
        "display(param_vs_latency_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_vs_memory_img = Image(filename=downloaded_folder / \"pareto_non_embedding_params_vs_onnx_memory.png\")\n",
        "display(param_vs_memory_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latency_vs_memory_img = Image(filename=downloaded_folder / \"pareto_onnx_latency_vs_onnx_memory.png\")\n",
        "display(latency_vs_memory_img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show search state of the last iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(downloaded_folder / \"search_state_5.csv\")\n",
        "\n",
        "styled_table = df.style.set_properties(**{'background-color': 'lightblue',\n",
        "                                          'color': 'black',\n",
        "                                          'border-color': 'white',\n",
        "                                          'font-size': '12pt'\n",
        "                                         })\n",
        "\n",
        "html_table = styled_table.to_html()\n",
        "html_with_scrollbar = f'<div style=\"height: 300px; overflow-y: scroll;\">{html_table}</div>'\n",
        "\n",
        "display(HTML(html_with_scrollbar))"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "archai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "0d4293aab49b039cc0cdcd865f65e47bb7838862d84c380ef42d6c53ee313261"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
