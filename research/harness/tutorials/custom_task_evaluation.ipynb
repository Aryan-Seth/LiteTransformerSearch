{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How-To Evaluate a Custom Task with LM-Eval\n",
    "\n",
    "Even though `lm-eval` framework supports more than 200 tasks, one might want to implement an additional one. With that in mind, this tutorial walks through the process of creating a custom task, including it in the registry and evaluating models with it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "The `harness` project is designed to be an installable module, which allow users to call it from outside its package. Thus, one can install it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import harness\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/microsoft/archai.git@pre-release#subdirectory=research/harness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Custom Task\n",
    "\n",
    "Tasks always inherits from the base class `Task`, which is implemented by the `lm_eval.base` module. When defining a custom task, there are some constants and methods that need to be overriden:\n",
    "\n",
    "### Constants\n",
    "\n",
    "* `VERSION`: Indicates the version of the task for reproducibility.\n",
    "* `DATASET_PATH`: Name of the dataset from the Hugging Face Hub.\n",
    "* `DATASET_NAME`: Configuration name of the dataset from the Hugging Face Hub.\n",
    "\n",
    "### Methods\n",
    "\n",
    "* `should_decontaminate()`: Whether can be decontaminated with an n-grams file.\n",
    "* `has_training_docs()`: Whether dataset supports a training set.\n",
    "* `has_validation_docs()`: Whether dataset supports a validation set.\n",
    "* `has_test_docs()`: Whether dataset supports a testing set.\n",
    "* `test_docs()`: Indicates the `DatasetDict` key to be used for the testing samples.\n",
    "* `doc_to_text()`: Defines the task input.\n",
    "* `doc_to_target()`: Defines the task target.\n",
    "* `construct_requests()`: Creates a tuple of requests that defines the core computation of the task (e.g., usually zero-shot is conducted using log-likelihood over the desired target token).\n",
    "* `process_results()`: Processes the output of the requests and calculates their metric (e.g., accuracy).\n",
    "* `aggregation()`: Defines how multiple outputs should be aggregated (e.g., mean).\n",
    "* `higher_is_better()`: Defines if a higher metric value corresponds to a better metric.\n",
    "\n",
    "*One can refer to the `lm-eval` implemented tasks if additional information is needed: https://github.com/EleutherAI/lm-evaluation-harness/tree/master/lm_eval/tasks.*\n",
    "\n",
    "In this example, we will be implementing the AX-b task from the SuperGLUE benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from harness.utils.request_factory import Request, rf\n",
    "from lm_eval.base import Task\n",
    "from lm_eval.metrics import mean\n",
    "\n",
    "class AXb(Task):\n",
    "    VERSION = 0\n",
    "    DATASET_PATH = \"super_glue\"\n",
    "    DATASET_NAME = \"axb\"\n",
    "\n",
    "    def should_decontaminate(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def has_training_docs(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def has_validation_docs(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def has_test_docs(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def test_docs(self) -> Dataset:\n",
    "        return self.dataset[\"test\"]\n",
    "\n",
    "    def doc_to_text(self, doc: Dict[str, Any]) -> str:\n",
    "        return f\"{doc['sentence1']}\\nQuestion: {doc['sentence2']} True or False?\\nAnswer:\"\n",
    "\n",
    "    def doc_to_target(self, doc: Dict[str, Any]) -> str:\n",
    "        available_labels = {0: \"True\", 1: \"False\"}\n",
    "        label = doc[\"label\"]\n",
    "\n",
    "        return f\" {available_labels[label]}\"\n",
    "\n",
    "    def construct_requests(self, doc: Dict[str, Any], ctx: str) -> List[Request]:\n",
    "        ll_true = rf.loglikelihood(ctx, \" True\")\n",
    "        ll_false = rf.loglikelihood(ctx, \" False\")\n",
    "\n",
    "        return ll_true, ll_false\n",
    "\n",
    "    def process_results(self, doc: Dict[str, Any], results: List[str]) -> Dict[str, Any]:\n",
    "        ll_true, ll_false = results\n",
    "\n",
    "        prediction = int(ll_false > ll_true)\n",
    "        reference = doc[\"label\"]\n",
    "\n",
    "        acc = 1.0 if (ll_true > ll_false) == reference else 0.0\n",
    "\n",
    "        return {\"acc\": acc}\n",
    "\n",
    "    def aggregation(self) -> Dict[str, Any]:\n",
    "        return {\"acc\": mean}\n",
    "\n",
    "    def higher_is_better(self) -> Dict[str, Any]:\n",
    "        return {\"acc\": True}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Task to Registry\n",
    "\n",
    "After a custom task has been defined, it needs to be added to two constants that enables its usability:\n",
    "\n",
    "* `ALL_TASKS`: List of available tasks (useful when parsing from the command line).\n",
    "* `TASK_REGISTRY`: Dictionary mapping the task identifier and its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.tasks import ALL_TASKS, TASK_REGISTRY\n",
    "\n",
    "ALL_TASKS.append(\"axb\")\n",
    "TASK_REGISTRY.update({\"axb\": AXb})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using Custom Task\n",
    "\n",
    "Finally, the custom task evaluation follows the same protocol defined by the `simple_evaluation.ipynb` example, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (C:\\Users\\gderosa\\.cache\\huggingface\\datasets\\super_glue\\axb\\1.0.2\\d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0790264162b43a3a79ccf5f1c76ebae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2200/2200 [09:33<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Task|Version|Metric|Value |   |Stderr|\n",
      "|----|------:|------|-----:|---|-----:|\n",
      "|axb |      0|acc   |0.5652|±  |0.0149|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from lm_eval.evaluator import make_table\n",
    "\n",
    "from harness.lm_eval_evaluator import evaluate_wrapper\n",
    "from harness.lm_eval_hf_model import HFEvalModel\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "hf_model = HFEvalModel(model, tokenizer)\n",
    "\n",
    "outputs = evaluate_wrapper(\n",
    "        hf_model,\n",
    "        [\"axb\"],\n",
    "        num_fewshot=0,\n",
    "        no_cache=True,\n",
    "    )\n",
    "\n",
    "print(make_table(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2be478cf8a2d9a6a1293b022e8589530f7ec0d0340a3a36da6068ef3d344086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
